# -----------------
# Preprocessing
# -----------------
Preprocessing:
  autoreject: !Categorical { choices: [ true, false ] }
  input_length: !Categorical { choices: [ 5, 10 ] }
  sfreq_multiple: !Categorical { choices: [ 2, 3 ] }

# -----------------
# Domain adaptation
# -----------------
# Domain discriminator
Discriminators:
  NoDiscriminator: {}
  ExponentialDecalFC:
    proposed_depth: {low: 1, high: 5, log: false}
    first_layer_multiple: {low: 0.5, high: 2, log: false}
    exponential_decrease: {low: 2, high: 5, log: false}
    activation_function:
      relu: {}
      elu: {low: 0.2, high: 1.8, log: false}

# Convolutional Monge mapping normalisation
# todo

# -----------------
# Training
# -----------------
Training:
  learning_rate: !Float { low: 0.000001, high: 0.01, step: null, log: true }
  beta_1: !Float { low: 0.7, high: 1.0, step: null, log: false }
  beta_2: !Float { low: 0.7, high: 1.0, step: null, log: false }
  eps: !Float { low: 0.0000000001, high: 0.000001, step: null, log: true }
Loss:
  loss: { choices: [ MSELoss, L1Loss ] }
  weighter: { choices: [ null, SamplePowerWeighter ] }
  weighter_kwargs:
    weight_power: !Float { low: 0, high: 1, step: null, log: false }

# -----------------
# DL architectures
# -----------------
DLArchitectures:
  InceptionNetwork:
    num_classes: 1
    cnn_units: { low: 4, high: 64, log: true }  # { low: 8, high: 64, log: true }
    # depth: { low: 1, high: 14, log: true }  # Remember that this value will be multiplied by 3
    depth: { low: 1, high: 3, log: true }  # Remember that this value will be multiplied by 3
  ShallowFBCSPNetMTS:
    num_classes: 1
    num_time_steps: UNAVAILABLE
    num_filters: { low: 20, high: 60, step: 1, log: false }
    filter_time_length: { low: 5, high: 45, step: 1, log: false }
    pool_time_stride: { low: 5, high: 25, step: 1, log: false }
    drop_prob: { low: 0.0, high: 0.5, step: null, log: false }
  Deep4NetMTS:
    num_classes: 1
    num_time_steps: UNAVAILABLE
    num_first_filters: { low: 10, high: 40, step: 1, log: false }
    filter_length: { low: 5, high: 15, step: 1, log: false }
    drop_prob: { low: 0.0, high: 0.5, log: false }  # todo: should this be independent from shallownet?
  TCNMTS:
    num_classes: 1
    n_blocks: { low: 1, high: 5, step: 1, log: false }
    n_filters:  { low: 4, high: 128, step: 1, log: True }
    kernel_size: { low: 3, high: 8, step: 1, log: False }
    drop_prob: { low: 0.0, high: 0.5, log: false }
  GreenModel:
    # General requirements
    num_classes: 1
    sampling_freq: UNAVAILABLE
    # Parameterised convolutions
    # n_freqs: { low: 1, high: 32, step: 1, log: true }
    n_freqs: { low: 1, high: 12, step: 1, log: true }
    kernel_width_s: { low: 2, high: 4, step: null, log: false }  # todo: how to sample this?
    conv_stride: { low: 1, high: 5, step: 1, log: false }
    oct_min: 0.0  # { low: 0.0, high: 0.2, step: null, log: false }
    oct_max_addition: 5.5  # { low: 5.5, high: 5.7, step: null, log: false }
    random_f_init: { choices: [ False, True ] }
    # Pooling layer
    pool_layer: { choices: [ RealCovariance, CrossCovariance, PW_PLV, CrossPW_PLV, CombinedPoolingNoCross,
                             CombinedPoolingCross ] }
    pool_layer_kwargs:
      RealCovariance: &RC {}
      CrossCovariance: &CC {}
      PW_PLV: &PWPLV { reg: { low: 0.0000001, high: 0.00001, log: true } }
      CrossPW_PLV: &CPWPLV { reg: { low: 0.0000001, high: 0.00001, log: true } }
      CombinedPoolingNoCross: { RealCovariance: *RC, PW_PLV: *PWPLV }
      CombinedPoolingCross: { CrossCovariance: *CC, CrossPW_PLV: *CPWPLV }
    # Shrinkage layer
    shrinkage_init: { low: -5.0, high: -1.0, step: null, log: false }
    # BiMap layer  todo: in the documentation it seems like it should be int, be code looked like iterable of ints
    bi_out_perc: { low: 0.5, high: 1, step: null, log: false }
    orth_weights: { choices: [ false ] }  # todo: setting to True makes current method for saving impossible
    # Combined ReEig and LogMap layer
    logref: { choices: [ identity, logeuclid ] }
    reeig_reg: { low: 0.00001, high: 0.001, step: null, log: true }  # todo: can be None in the code too...
    momentum: { low: 0.8, high: 1.0, step: null, log: false }
    # Fully connected module
    num_fc_layers: { low: 1, high: 3, step: 1, log: false }
    num_first_fc_filters: { low: 16, high: 128, step: 1, log: true }
    drop_prob: { low: 0.0, high: 0.5, log: false }
normalisation: { choices: [ true, false ] }

# -----------------
# HPs for varied numbers of channels
# -----------------
Interpolation:
  # todo: not elegant, consider inferring these at runtime
  main_channel_system: { choices: [ LEMON, Wang, DortmundVital ] }
  methods: { choices: [ MNE, spline ] }

_num_kernels: &num_kernels !Int { low: 10, high: 100, log: true }
_max_receptive_field: &max_receptive_field !Int { low: 15, high: 50, log: true }  # { low: 15, high: 100, log: true }
RegionBasedPooling:
  num_montage_splits: { low: 1, high: 8, log: true }
  share_all_pooling_modules: { choices: [ true, false ] }
  num_pooling_modules_percentage: { low: 0.0, high: 1.0, step: null, log: false }
  num_designs: 1
  pooling_type: multi_cs  # this shouldn't be necessary
  PoolingMethods:
    MultiMSMean: {}
    MultiMSSharedRocket:
      num_kernels: *num_kernels
      max_receptive_field: *max_receptive_field
    MultiMSSharedRocketHeadRegion:
      num_kernels: *num_kernels
      max_receptive_field: *max_receptive_field
      latent_search_features: !Int { low: 8, high: 64, log: true }
      share_search_receiver_modules: !Categorical { choices: [ true, false ] }
      bias: !Categorical { choices: [ false ] }
  MontageSplits:
    CentroidPolygons:
      k: !CategoricalDict
        k_1: [ 2, 2, 2, 2, 2, 2, 2 ]
        k_2: [ 3, 3, 3, 3, 3, 3, 3, 3 ]
        k_3: [ 2, 3, 2, 3, 2, 3, 2, 3, 2 ]
        k_4: [ 4, 3, 2, 3, 4, 3, 2, 3, 4 ]
      min_nodes: !Int { low: 1, high: 6, step: 1, log: false }
      channel_positions: !NotAHyperparameterList
        [ LEMON, Wang, DortmundVital ]

SpatialDimensionMismatch: { choices: [ Interpolation, RegionBasedPooling ] }
