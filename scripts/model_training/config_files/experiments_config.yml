# All keys which do start with 'Pretext' or 'Downstream' will not be used in AllHPOExperiments. Current version is
# case-insensitive

_main_metric: &main_metric r2_score
# -----------------
# Datasets
# -----------------
_LEMON: &LEMON
  num_subjects: all
  num_time_steps: null
  time_series_start: null
  channels: null
_DortmundVital: &DortmundVital
  num_subjects: all
  num_time_steps: null
  time_series_start: null
  channels: null

DownstreamDatasets: &DownstreamDatasets
  DortmundVital: *DortmundVital
PretextDatasets: &PretextDatasets
  LEMON: *LEMON
  DortmundVital: *DortmundVital

# -----------------
# Configuration for subject splitting
# -----------------
_split_seed: &split_seed 42
_test_set_percentage: &test_set_percentage 0.2

DownstreamSubjectSplit:
  val_split: 0.2  # This percentage operates on the subjects AFTER separating out the test set
  test_split: *test_set_percentage
  seed: *split_seed
  normal_num_random_splits: &num_random_splits 1
  elecssl_num_random_splits: 50
DOWNSTREAMval_scores_aggregation: median  # To use median or average of folds performance as signal to HPO algorithm.
# However, as we are using one fold only, mean and median are the same


PretextMLModelSettings:  # todo: this is Elecssl only, not pretraining. Better to make a separate one I think
  aggregation_method: median
  evaluation_metric: *main_metric
  metrics: regression
  test_prediction_aggregation: mean
  test_predictions_decimals: 4
  test_scores_decimals: 4
PretextSubjectSplit:  # This is for the actual pretext tasks
  left_out_datasets: !GetKeys { <<: *DownstreamDatasets }
  val_split: 0.2
  num_random_splits: *num_random_splits
  seed: *split_seed

# -----------------
# Target scaling
# -----------------
_Scalers: &Scaler
  target:
    kwargs: { }
    name: ZNormalisation

DownstreamScalers: *Scaler
PretextScalers: *Scaler

# -----------------
# Training configurations
# -----------------
_Training: &Training
  batch_size: 64
  num_epochs: 100
  verbose: true
  verbose_variables: false
  continuous_testing: false  # too time-consuming I think, not worth it
  metrics: regression
  prediction_activation_function: null
  main_metric: *main_metric
DownstreamTraining:
  target: age
  <<: *Training
PretextTraining:  # The pseudo-target is added at run time
  log_transform_targets: log10
  <<: *Training

# -----------------
# Configurations for computing metrics
# -----------------
DownstreamSubGroups:
  sub_groups:
    dataset_name: !GetKeys { <<: *DownstreamDatasets }
  verbose: false
PretextSubGroups:
  sub_groups:
    dataset_name: !GetKeys { <<: *PretextDatasets }
  verbose: false